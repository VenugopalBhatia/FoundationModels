{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-05 17:43:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8001::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.1’\n",
      "\n",
      "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-11-05 17:43:26 (11.6 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "charset = sorted(list(set(data)))\n",
    "vocab_size=  len(charset)\n",
    "\n",
    "print(''.join(charset),vocab_size,sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]\n",
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i']\n"
     ]
    }
   ],
   "source": [
    "# encode and decode using serialization\n",
    "stoi = {ch:i for i,ch in enumerate(charset)}\n",
    "itos = {i:ch for i,ch in enumerate(charset)}\n",
    "\n",
    "encoder  = lambda s: [stoi[ch] for ch in s]\n",
    "decoder = lambda n: [itos[i] for i in n]\n",
    "\n",
    "\n",
    "encoded = encoder(data[:10])\n",
    "decoded = decoder(encoded)\n",
    "print(data[:10],encoded,decoded,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# enc = encoder.encode(data[:10])\n",
    "# dec = encoder.decode(enc)\n",
    "\n",
    "# print(enc,dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56,  ..., 45,  8,  0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkns = torch.tensor(encoder(data))\n",
    "\n",
    "tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5451, 47317,   512,  ...,  1989, 48728,   627])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(tkns))\n",
    "train_data = tkns[:n+1]\n",
    "val_data = tkns[n+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58, 46, 43, 56,  1, 51, 43,  6],\n",
      "        [54, 53, 47, 57, 53, 52, 43, 56],\n",
      "        [33, 15, 23, 21, 26, 19, 20, 13],\n",
      "        [21, 17, 32, 10,  0, 37, 53, 52]])\n",
      "tensor([[46, 43, 56,  1, 51, 43,  6,  1],\n",
      "        [53, 47, 57, 53, 52, 43, 56,  0],\n",
      "        [15, 23, 21, 26, 19, 20, 13, 25],\n",
      "        [17, 32, 10,  0, 37, 53, 52,  1]])\n",
      "when input is tensor([58]), output is 46\n",
      "when input is tensor([58, 46]), output is 43\n",
      "when input is tensor([58, 46, 43]), output is 56\n",
      "when input is tensor([58, 46, 43, 56]), output is 1\n",
      "when input is tensor([58, 46, 43, 56,  1]), output is 51\n",
      "when input is tensor([58, 46, 43, 56,  1, 51]), output is 43\n",
      "when input is tensor([58, 46, 43, 56,  1, 51, 43]), output is 6\n",
      "when input is tensor([58, 46, 43, 56,  1, 51, 43,  6]), output is 1\n",
      "when input is tensor([54]), output is 53\n",
      "when input is tensor([54, 53]), output is 47\n",
      "when input is tensor([54, 53, 47]), output is 57\n",
      "when input is tensor([54, 53, 47, 57]), output is 53\n",
      "when input is tensor([54, 53, 47, 57, 53]), output is 52\n",
      "when input is tensor([54, 53, 47, 57, 53, 52]), output is 43\n",
      "when input is tensor([54, 53, 47, 57, 53, 52, 43]), output is 56\n",
      "when input is tensor([54, 53, 47, 57, 53, 52, 43, 56]), output is 0\n",
      "when input is tensor([33]), output is 15\n",
      "when input is tensor([33, 15]), output is 23\n",
      "when input is tensor([33, 15, 23]), output is 21\n",
      "when input is tensor([33, 15, 23, 21]), output is 26\n",
      "when input is tensor([33, 15, 23, 21, 26]), output is 19\n",
      "when input is tensor([33, 15, 23, 21, 26, 19]), output is 20\n",
      "when input is tensor([33, 15, 23, 21, 26, 19, 20]), output is 13\n",
      "when input is tensor([33, 15, 23, 21, 26, 19, 20, 13]), output is 25\n",
      "when input is tensor([21]), output is 17\n",
      "when input is tensor([21, 17]), output is 32\n",
      "when input is tensor([21, 17, 32]), output is 10\n",
      "when input is tensor([21, 17, 32, 10]), output is 0\n",
      "when input is tensor([21, 17, 32, 10,  0]), output is 37\n",
      "when input is tensor([21, 17, 32, 10,  0, 37]), output is 53\n",
      "when input is tensor([21, 17, 32, 10,  0, 37, 53]), output is 52\n",
      "when input is tensor([21, 17, 32, 10,  0, 37, 53, 52]), output is 1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "ix = torch.randint(len(tkns)-batch_size,(batch_size,))\n",
    "\n",
    "def get_batch(batch_size,block_size,data_split = \"train\"):\n",
    "    data = train_data if data_split == \"train\" else val_data\n",
    "    ix = torch.randint(len(tkns)-block_size,(batch_size,))\n",
    "    input_data = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    target = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "\n",
    "    return input_data,target\n",
    "\n",
    "def visualize(input_data,target):\n",
    "    for ix in range(len(input_data)):\n",
    "        for i in range(block_size):\n",
    "            inpt = input_data[ix,:i+1]\n",
    "            otpt = target[ix,i]\n",
    "            print('when input is {}, output is {}'.format(inpt,otpt))\n",
    "\n",
    "i,o = get_batch(batch_size=batch_size,block_size=block_size)\n",
    "print(i)\n",
    "print(o)\n",
    "\n",
    "# print(i.shape,i[1,:])\n",
    "visualize(i,o)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11604,   198,  2409,   293,  1604,  1285, 70535,   323],\n",
       "        [  499,  2643,   617,  9160,  2454,   264,   628,  5827],\n",
       "        [49972,   382,    43,  5576,  3895,  1473, 81361,   537],\n",
       "        [  345,  3112, 15411,  1057, 13162,   304,  3776, 76350]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(76350)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "# vocab_size = encoder.n_vocab\n",
    "\n",
    "embeddings = torch.nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "emb = embeddings(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0714,  0.8801,  0.1912,  ...,  0.1745,  0.9324,  0.0393],\n",
       "         [ 1.6723,  0.5173,  0.9703,  ..., -0.6564,  1.7701, -0.5448],\n",
       "         [ 1.0838, -0.0849, -1.6914,  ...,  0.6126, -1.4797,  0.3492],\n",
       "         ...,\n",
       "         [ 2.3328, -1.3859,  1.0154,  ..., -1.1096,  1.1374, -0.9502],\n",
       "         [ 1.0838, -0.0849, -1.6914,  ...,  0.6126, -1.4797,  0.3492],\n",
       "         [ 0.6816,  1.5178, -0.6353,  ...,  0.1027, -0.4830, -0.2608]],\n",
       "\n",
       "        [[-1.9330,  1.1050, -0.1408,  ..., -0.0831, -1.6705, -0.4293],\n",
       "         [-1.0829, -0.9296,  1.5269,  ..., -0.4009, -0.3732, -0.7805],\n",
       "         [ 1.3584,  0.4848,  1.2366,  ...,  0.8587, -2.4105,  0.8027],\n",
       "         ...,\n",
       "         [-1.5136, -1.1765, -0.7252,  ..., -0.1077, -0.0091,  0.7436],\n",
       "         [ 1.0838, -0.0849, -1.6914,  ...,  0.6126, -1.4797,  0.3492],\n",
       "         [ 0.3418, -0.2136, -0.2575,  ...,  0.8734, -1.5082, -2.1618]],\n",
       "\n",
       "        [[-1.2984, -0.1949, -0.4208,  ..., -1.0695, -2.0217,  0.7896],\n",
       "         [ 1.2397, -1.0817, -0.3302,  ..., -0.4418, -0.3449, -1.7529],\n",
       "         [-0.7644,  0.9730,  0.2234,  ...,  0.0282, -0.2703, -0.2608],\n",
       "         ...,\n",
       "         [ 0.2621, -0.0980,  0.2789,  ...,  1.0428, -0.4526,  0.2269],\n",
       "         [ 0.3287,  0.7297,  1.9877,  ..., -0.6535, -1.3296,  0.2188],\n",
       "         [ 0.9686, -1.5552,  0.7989,  ..., -0.7910,  0.4084,  0.5482]],\n",
       "\n",
       "        [[ 0.4604, -1.1690, -0.4825,  ...,  0.6764, -0.9395, -0.1292],\n",
       "         [ 1.4224,  0.1385, -0.1659,  ..., -0.0638,  0.5967,  1.0025],\n",
       "         [-0.2829, -0.5610,  0.3582,  ..., -0.2559, -0.3951,  1.1185],\n",
       "         ...,\n",
       "         [ 0.7446,  1.5462, -0.1867,  ..., -0.7415,  2.3651,  0.7307],\n",
       "         [-1.0829, -0.9296,  1.5269,  ..., -0.4009, -0.3732, -0.7805],\n",
       "         [-1.5136, -1.1765, -0.7252,  ..., -0.1077, -0.0091,  0.7436]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9801,  0.6824, -0.7586, -0.5210,  0.3097, -0.8427,  1.3237, -0.7553,\n",
      "        -0.3777, -0.6922, -0.6532, -0.2703,  1.9668, -1.4623,  0.2717, -1.1529,\n",
      "        -0.0772,  0.2818,  1.5907,  0.2737,  0.8273, -0.6517, -2.5686,  1.3187,\n",
      "         0.3632, -0.7837,  0.5977,  0.6139,  1.9707, -0.2860, -0.8189,  0.6200,\n",
      "        -2.7965,  0.3460,  0.3316, -0.9157, -0.7333,  0.6930, -1.4625, -0.9355,\n",
      "        -0.5180, -1.1246,  0.6485,  0.7721,  0.3665,  1.2456, -0.3826, -0.1085,\n",
      "        -0.0827, -0.8512, -0.2678,  0.8035, -0.4030, -0.8973, -0.6216, -0.5001,\n",
      "        -0.9062, -1.4205, -0.0802, -0.1577,  0.0168, -1.3619,  0.8723, -0.1968,\n",
      "        -0.1024], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings(torch.tensor([64])).view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vocab_size = 65\n",
    "        self.embedding = torch.nn.Embedding(vocab_size,vocab_size)\n",
    "    \n",
    "    def forward(self,inpts,targets=None):\n",
    "        logits = self.embedding(inpts)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # print(logits)\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            #loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "            #loss = loss_fcn(target, logits)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate_tkns(self,inpts,max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(inpts)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            inpts = torch.cat((inpts, idx_next), dim=1) # (B, T+1)\n",
    "        return inpts\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.3858, -0.4780,  0.4837,  ...,  1.4251, -0.9275,  0.8669],\n",
      "        [-0.2404, -0.4747, -0.4338,  ..., -0.7907,  1.8102,  0.8979],\n",
      "        [ 0.2246,  1.5730, -0.6026,  ...,  0.2024, -0.1892,  0.6314],\n",
      "        ...,\n",
      "        [-0.1428,  0.1078, -1.4439,  ...,  1.3444,  4.9990, -0.1206],\n",
      "        [ 1.6918, -0.6464,  0.9655,  ..., -1.4093,  0.5757,  0.6651],\n",
      "        [-0.8869,  2.1616, -1.1989,  ..., -2.0739, -1.0880,  2.3583]],\n",
      "       grad_fn=<ViewBackward0>), tensor(4.3023, grad_fn=<NllLossBackward0>))\n",
      "\n",
      "wx-,&wBuVEoorPbNHy?L3v\n",
      "heXZJ$3uc:\n",
      "RU\n",
      "RQfnzNDM!fnYxZpfi:JT-VD\n",
      "PZyZN\n",
      "OxvGpUQs?3pHThO'I&kQW.AXwjsESndyy\n"
     ]
    }
   ],
   "source": [
    "m = BigramModel()\n",
    "print(m(i,o))\n",
    "m.generate_tkns(inpts = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()\n",
    "print(''.join(decoder(m.generate_tkns(inpts = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramModel(\n",
       "  (embedding): Embedding(65, 65)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
